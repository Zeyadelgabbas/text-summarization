{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817e16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb793b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Aiprojects\\\\Textsummarization\\\\text-summarization\\\\Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa23909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Aiprojects\\\\Textsummarization\\\\text-summarization'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68190c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    #config \n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: str\n",
    "    #params\n",
    "    num_train_epochs: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    learning_rate: float\n",
    "    warmup_steps: int\n",
    "    weight_decay: float\n",
    "    eval_strategy: str\n",
    "    save_strategy: str\n",
    "    save_total_limit: int\n",
    "    logging_steps: int\n",
    "    eval_steps: int\n",
    "    load_best_model_at_end: bool\n",
    "    metric_for_best_model: str\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4428a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import * \n",
    "from src.utils import read_yaml,create_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed93ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,config_filepath = CONFIG_FILE_PATH , params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directory([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) ->ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingParameters\n",
    "\n",
    "        create_directory([config.root_dir])\n",
    "        config = ModelTrainerConfig(\n",
    "                    root_dir = config.root_dir,\n",
    "                    data_path = config.data_path,\n",
    "                    model_ckpt = config.model_ckpt,\n",
    "                    num_train_epochs = params.num_train_epochs,\n",
    "                    per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "                    per_device_eval_batch_size = params.per_device_eval_batch_size,\n",
    "                    learning_rate = params.learning_rate,\n",
    "                    warmup_steps = params.warmup_steps,\n",
    "                    weight_decay = params.weight_decay,\n",
    "                    eval_strategy = params.eval_strategy,\n",
    "                    save_strategy = params.save_strategy,\n",
    "                    save_total_limit = params.save_total_limit,\n",
    "                    logging_steps = params.logging_steps,\n",
    "                    eval_steps = params.eval_steps,\n",
    "                    load_best_model_at_end = params.load_best_model_at_end,\n",
    "                    metric_for_best_model = params.metric_for_best_model,\n",
    "                    gradient_accumulation_steps = params.gradient_accumulation_steps,\n",
    "                )\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadae986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Aiprojects\\Textsummarization\\text-summarization\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM , Trainer , TrainingArguments , DataCollatorForSeq2Seq\n",
    "from datasets import load_from_disk\n",
    "from src.logging.logger import get_logger\n",
    "from src.Exception import CustomException\n",
    "import sys\n",
    "from dataclasses import asdict\n",
    "from src.utils import get_safe_batch_size\n",
    "logging = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c49998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,config:ModelTrainerConfig):\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer , model = model_pegasus)\n",
    "        dataset = load_from_disk(self.config.data_path)\n",
    "\n",
    "        dic_debug = asdict(self.config)\n",
    "\n",
    "        for key, value in dic_debug.items():\n",
    "            print(key, value, type(value))\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    learning_rate = self.config.learning_rate,\n",
    "    warmup_steps = self.config.warmup_steps,\n",
    "    weight_decay = self.config.weight_decay,\n",
    "    eval_strategy = self.config.eval_strategy,\n",
    "    save_strategy = self.config.save_strategy,\n",
    "    save_total_limit = self.config.save_total_limit,\n",
    "    logging_steps = self.config.logging_steps,\n",
    "    eval_steps = self.config.eval_steps,\n",
    "    load_best_model_at_end = self.config.load_best_model_at_end,\n",
    "    metric_for_best_model = self.config.metric_for_best_model,\n",
    "    gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n",
    "    num_train_epochs = self.config.num_train_epochs,\n",
    "    output_dir = self.config.root_dir,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "        trainer = Trainer(model = model_pegasus , args = training_args ,\n",
    "                           tokenizer = tokenizer , data_collator=data_collator ,\n",
    "                           train_dataset=dataset[\"train\"],\n",
    "                           eval_dataset=dataset[\"validation\"]\n",
    "                           )\n",
    "        trainer.train()\n",
    "        logging.info(\"model training finished\")\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n",
    "        logging.info(\"model and tokenizer saved\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "044f7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\LOQ\\AppData\\Local\\Temp\\ipykernel_23336\\3498309140.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model = model_pegasus , args = training_args ,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir artifacts/model_trainer <class 'str'>\n",
      "data_path artifacts/data_transformation/samsum_dataset_transformed <class 'str'>\n",
      "model_ckpt google/pegasus-cnn_dailymail <class 'str'>\n",
      "num_train_epochs 1 <class 'int'>\n",
      "per_device_train_batch_size 1 <class 'int'>\n",
      "per_device_eval_batch_size 1 <class 'int'>\n",
      "learning_rate 0.0001 <class 'float'>\n",
      "warmup_steps 200 <class 'int'>\n",
      "weight_decay 0.01 <class 'float'>\n",
      "eval_strategy steps <class 'str'>\n",
      "save_strategy steps <class 'str'>\n",
      "save_total_limit 3 <class 'int'>\n",
      "logging_steps 10 <class 'int'>\n",
      "eval_steps 100 <class 'int'>\n",
      "load_best_model_at_end True <class 'bool'>\n",
      "metric_for_best_model rouge2 <class 'str'>\n",
      "gradient_accumulation_steps 16 <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/921 19:29 < 2:41:24, 0.08 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.922600</td>\n",
       "      <td>1.648633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CustomException",
     "evalue": "Error in [C:\\Users\\LOQ\\AppData\\Local\\Temp\\ipykernel_23336\\1696899842.py] , line : [5] : \"The `metric_for_best_model` training argument is set to 'eval_rouge2', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss']. Consider changing the `metric_for_best_model` via the TrainingArguments.\" ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aiprojects\\Textsummarization\\text-summarization\\venv\\Lib\\site-packages\\transformers\\trainer.py:3296\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3295\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3296\u001b[39m     metric_value = \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3297\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mKeyError\u001b[39m: 'eval_rouge2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      4\u001b[39m     model_trainer = ModelTrainer(config=model_trainer_config)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mmodel_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e :\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m trainer = Trainer(model = model_pegasus , args = training_args ,\n\u001b[32m     39\u001b[39m                    tokenizer = tokenizer , data_collator=data_collator ,\n\u001b[32m     40\u001b[39m                    train_dataset=dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     41\u001b[39m                    eval_dataset=dataset[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     42\u001b[39m                    )\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mmodel training finished\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aiprojects\\Textsummarization\\text-summarization\\venv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aiprojects\\Textsummarization\\text-summarization\\venv\\Lib\\site-packages\\transformers\\trainer.py:2754\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2753\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2754\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aiprojects\\Textsummarization\\text-summarization\\venv\\Lib\\site-packages\\transformers\\trainer.py:3228\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3227\u001b[39m metrics = \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval)\n\u001b[32m-> \u001b[39m\u001b[32m3228\u001b[39m is_new_best_metric = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_determine_best_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aiprojects\\Textsummarization\\text-summarization\\venv\\Lib\\site-packages\\transformers\\trainer.py:3298\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3297\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m-> \u001b[39m\u001b[32m3298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   3299\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, which is not found in the evaluation metrics. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3300\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3301\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m   3303\u001b[39m operator = np.greater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.greater_is_better \u001b[38;5;28;01melse\u001b[39;00m np.less\n",
      "\u001b[31mKeyError\u001b[39m: \"The `metric_for_best_model` training argument is set to 'eval_rouge2', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss']. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCustomException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m error = CustomException(e,sys)\n\u001b[32m      9\u001b[39m logging.error(error)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[31mCustomException\u001b[39m: Error in [C:\\Users\\LOQ\\AppData\\Local\\Temp\\ipykernel_23336\\1696899842.py] , line : [5] : \"The `metric_for_best_model` training argument is set to 'eval_rouge2', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss']. Consider changing the `metric_for_best_model` via the TrainingArguments.\" "
     ]
    }
   ],
   "source": [
    "try : \n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.train()\n",
    "\n",
    "except Exception as e :\n",
    "    error = CustomException(e,sys)\n",
    "    logging.error(error)\n",
    "    raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f362947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bac93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
